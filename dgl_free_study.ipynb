{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Set Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = dict()\n",
    "\n",
    "# 1. Prepare Data\n",
    "dataset_args['file']                    = r'C:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\data\\steady_cavity_case_b200_maxU100ms_simple_normalized.npy'\n",
    "dataset_args['percent split (decimal)'] = 0.7\n",
    "dataset_args['randomizer seed']         = 42\n",
    "dataset_args['use-normalizer']          = 'unit'\n",
    "dataset_args['normalize_x']             = 'unit'\n",
    "dataset_args['cell to pointwise']       = True\n",
    "dataset_args['add boundaries']          = True\n",
    "dataset_args['subsampler'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (200, 65, 65, 3), subsampled by 4\n",
      "Dataset Split up using torch generator seed: 42\n",
      "              This can be replicated e.g.\n",
      "                generator_object = torch.Generator().manual_seed(42)\n",
      " \n",
      "torch.Size([65, 65])\n",
      "Queries torch.Size([4225, 2]) Coordinates torch.Size([4225, 1])\n",
      "Target features are normalized using unit transformer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\dgl\\heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[ 0.0093,  0.0004, -0.0328]]) tensor([[0.2326, 0.1816, 0.1040]])\n",
      "Target features are normalized using unit transformer\n",
      "Input features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "from data_storage.cavity_2d_data_handling import Cavity_2D_dataset_handling_v2\n",
    "from data_utils import MIODataLoader, WeightedLpLoss, LpLoss\n",
    "dgl_dataset = Cavity_2D_dataset_handling_v2(dataset_args['file'], name='cavity', train=True, sub_x = dataset_args['subsampler'],\n",
    "                                    normalize_y=dataset_args['use-normalizer'], normalize_x = dataset_args['normalize_x'],\n",
    "                                    data_split = dataset_args['percent split (decimal)'], seed = dataset_args['randomizer seed'],\n",
    "                                    vertex = dataset_args['cell to pointwise'], boundaries = dataset_args['add boundaries']\n",
    "                                    )\n",
    "dgl_dataloader = MIODataLoader(dgl_dataset, batch_size=4, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dim': 2, 'theta_dim': 1, 'output_dim': 3, 'branch_sizes': [1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl_dataset.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dgl_batch in dgl_dataloader:\n",
    "    g1, u_p1, g_u1 = dgl_batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what does the model see when the input is unpacked and feed into `block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1:  Graph(num_nodes=16900, num_edges=0,\n",
      "      ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.float32), 'y': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "step 2:  [Graph(num_nodes=4225, num_edges=0,\n",
      "      ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.float32), 'y': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=4225, num_edges=0,\n",
      "      ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.float32), 'y': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=4225, num_edges=0,\n",
      "      ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.float32), 'y': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=4225, num_edges=0,\n",
      "      ndata_schemes={'x': Scheme(shape=(2,), dtype=torch.float32), 'y': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={})]\n",
      "step 3:  4 example torch.Size([4225, 2])\n",
      "step 4:  torch.Size([4225, 4, 2])\n",
      "step 5:  torch.Size([4, 4225, 2])\n",
      "step 6:  torch.Size([4, 4225, 3])\n"
     ]
    }
   ],
   "source": [
    "print('step 1: ', g1)\n",
    "gs = dgl.unbatch(g1)\n",
    "print('step 2: ', gs)\n",
    "x = [_g.ndata['x'] for _g in gs]\n",
    "print('step 3: ', len(x), 'example', x[0].shape)\n",
    "x = torch.nn.utils.rnn.pad_sequence(x)\n",
    "print('step 4: ', x.shape)\n",
    "x = x.permute(1, 0, 2)  # B, T1, F\n",
    "print('step 5: ', x.shape)\n",
    "#x = torch.nn.utils.rnn.pad_sequence([_g.ndata['x'] for _g in gs]).permute(1, 0, 2)  # B, T1, F\n",
    "x = torch.cat([x, u_p1.unsqueeze(1).repeat([1, x.shape[1], 1])], dim=-1)\n",
    "print('step 6: ', x.shape)\n",
    "\n",
    "branch_sizes = dgl_dataset.config['branch_sizes']\n",
    "if branch_sizes:\n",
    "    n_inputs = len(branch_sizes)\n",
    "else: \n",
    "    n_inputs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though there are a few transformations. It looks like the batch is taken in quite normally with shape `[batch,nodes,channels]`.\n",
    "\n",
    "Perhaps it is the loss function that is causing a difference in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cgpt import CGPTNO\n",
    "# 2. Construct Model\n",
    "model_args = dict()\n",
    "model_args['trunk_size']        = dgl_dataset.config['input_dim']\n",
    "model_args['theta_size']        = dgl_dataset.config['theta_dim']\n",
    "model_args['branch_sizes']      = dgl_dataset.config['branch_sizes']\n",
    "\n",
    "model_args['output_size']         = 3\n",
    "model_args['n_layers']            = 3\n",
    "model_args['n_hidden']            = 128  \n",
    "model_args['n_head']              = 1\n",
    "model_args['attn_type']           = 'linear'\n",
    "model_args['ffn_dropout']         = 0.0\n",
    "model_args['attn_dropout']        = 0.0\n",
    "model_args['mlp_layers']          = 2\n",
    "model_args['act']                 = 'gelu'\n",
    "model_args['hfourier_dim']        = 0\n",
    "\n",
    "model_dgl = None\n",
    "model_dgl = CGPTNO(\n",
    "            trunk_size          = model_args['trunk_size'] + model_args['theta_size'],\n",
    "            branch_sizes        = model_args['branch_sizes'],     # No input function means no branches\n",
    "            output_size         = model_args['output_size'],\n",
    "            n_layers            = model_args['n_layers'],\n",
    "            n_hidden            = model_args['n_hidden'],\n",
    "            n_head              = model_args['n_head'],\n",
    "            attn_type           = model_args['attn_type'],\n",
    "            ffn_dropout         = model_args['ffn_dropout'],\n",
    "            attn_dropout        = model_args['attn_dropout'],\n",
    "            mlp_layers          = model_args['mlp_layers'],\n",
    "            act                 = model_args['act'],\n",
    "            horiz_fourier_dim   = model_args['hfourier_dim']\n",
    "            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16900, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model_dgl(g1, u_p1, g_u1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7.1:  before: torch.Size([16900, 3]) after: torch.Size([16900, 3])\n",
      "step 7.2:  before: torch.Size([16900, 3]) after: torch.Size([16900, 3])\n",
      "step 7.3:  loss: tensor(0.8750, grad_fn=<MeanBackward0>) reg: tensor(0.)\n",
      "step 7.4:  total loss: tensor(0.8750, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_func = WeightedLpLoss(p=2,component='all', normalizer=None)\n",
    "\n",
    "y_pred, y = out.squeeze(), g1.ndata['y'].squeeze()\n",
    "print('step 7.1: ', 'before:', out.shape, 'after:', y_pred.shape)\n",
    "print('step 7.2: ', 'before:', g1.ndata['y'].shape, 'after:', y.shape)\n",
    "\n",
    "# 5.2. Calculate Loss for Backwards Pass\n",
    "loss, reg,  _ = loss_func(g1, y_pred, y)\n",
    "print('step 7.3: ', 'loss:', loss, 'reg:', reg)\n",
    "loss_total = loss + reg\n",
    "print('step 7.4: ', 'total loss:', loss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO the output is concatenated along the batches. Let's see if the loss looks different if we change the layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "DGLError",
     "evalue": "Expect number of features to match number of nodes (len(u)). Got 4 and 16900 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDGLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3df07b1ae746>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 5.2. Calculate Loss for Backwards Pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step 8.1: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reg:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step 8.2: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'total loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\data_utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, g, pred, target)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m#### only for computing metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lp_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\data_utils.py\u001b[0m in \u001b[0;36m_lp_losses\u001b[1;34m(self, g, pred, target)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_lp_losses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'all'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\dgl\\nn\\pytorch\\glob.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, graph, feat)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \"\"\"\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"h\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             \u001b[0mreadout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"h\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreadout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\dgl\\view.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;34m\"please pass a tensor directly\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             )\n\u001b[1;32m---> 99\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_n_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ntid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__delitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\dgl\\heterograph.py\u001b[0m in \u001b[0;36m_set_n_repr\u001b[1;34m(self, ntid, u, data)\u001b[0m\n\u001b[0;32m   4340\u001b[0m             \u001b[0mnfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4341\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnfeats\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4342\u001b[1;33m                 raise DGLError(\n\u001b[0m\u001b[0;32m   4343\u001b[0m                     \u001b[1;34m\"Expect number of features to match number of nodes (len(u)).\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4344\u001b[0m                     \u001b[1;34m\" Got %d and %d instead.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDGLError\u001b[0m: Expect number of features to match number of nodes (len(u)). Got 4 and 16900 instead."
     ]
    }
   ],
   "source": [
    "# 5.2. Calculate Loss for Backwards Pass\n",
    "loss, reg,  _ = loss_func(g1, y_pred.reshape(4,65,65,3), y.reshape(4,65,65,3))\n",
    "print('step 8.1: ', 'loss:', loss, 'reg:', reg)\n",
    "loss_total = loss + reg\n",
    "print('step 8.2: ', 'total loss:', loss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope does not work due to their weird function. Lets see if we can replicate the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9:  total loss: tensor(0.8342, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = torch.nn.functional.mse_loss(y_pred, y, reduction = 'mean')\n",
    "print('step 9: ', 'total loss:', mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very similar. In the paper they use relative L2 norm as a metric. Let's see if that works too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10:  total loss: tensor(1.3175, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lploss_f = LpLoss()\n",
    "lploss = lploss_f.rel(y_pred, y)\n",
    "print('step 10: ', 'total loss:', lploss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all still in the relative ball-park of losses. However, we are still looking at a series of points (all batches concatenated). We want the loss to be independent of batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11:  total loss: tensor(0.8342, grad_fn=<MseLossBackward0>)\n",
      "step 12:  total loss: tensor(1.0384, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = torch.nn.functional.mse_loss(y_pred.reshape(4,65,65,3), y.reshape(4,65,65,3), reduction = 'mean')\n",
    "print('step 11: ', 'total loss:', mse_loss)\n",
    "lploss = lploss_f.rel(y_pred.reshape(4,65,65,3), y.reshape(4,65,65,3))\n",
    "print('step 12: ', 'total loss:', lploss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So MSE is independent of matrix structure but rel LP loss isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets change the batchsize and see if that has an effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 13:  total loss: tensor(0.8278, grad_fn=<MseLossBackward0>)\n",
      "step 14:  total loss: tensor(1.0400, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mse_loss = torch.nn.functional.mse_loss(y_pred.reshape(4,65,65,3)[:2,...], y.reshape(4,65,65,3)[:2,...], reduction = 'mean')\n",
    "print('step 13: ', 'total loss:', mse_loss)\n",
    "lploss = lploss_f.rel(y_pred.reshape(4,65,65,3)[:2,...], y.reshape(4,65,65,3)[:2,...])\n",
    "print('step 14: ', 'total loss:', lploss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really, that is a good sign! Question is though, why did the training not converge or break when using our dgl-free dataloader. Could it have been due to the normalization or the use of Reynolds number instead of Lid Velocities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets run it through the model and see if a reduced batchsize has an effect there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 15.1:  before: torch.Size([8450, 3]) after: torch.Size([8450, 3])\n",
      "step 15.2:  before: torch.Size([8450, 3]) after: torch.Size([8450, 3])\n",
      "step 15.3:  loss: tensor(0.8743, grad_fn=<MeanBackward0>) reg: tensor(0.)\n",
      "step 15.4:  total loss: tensor(0.8743, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dgl_dataloader2 = MIODataLoader(dgl_dataset, batch_size=2, shuffle=False, drop_last=False)\n",
    "\n",
    "for dgl_batch in dgl_dataloader2:\n",
    "    g1, u_p1, g_u1 = dgl_batch\n",
    "    break\n",
    "\n",
    "out = model_dgl(g1, u_p1, g_u1)\n",
    "\n",
    "y_pred, y = out.squeeze(), g1.ndata['y'].squeeze()\n",
    "print('step 15.1: ', 'before:', out.shape, 'after:', y_pred.shape)\n",
    "print('step 15.2: ', 'before:', g1.ndata['y'].shape, 'after:', y.shape)\n",
    "\n",
    "# 5.2. Calculate Loss for Backwards Pass\n",
    "loss, reg,  _ = loss_func(g1, y_pred, y)\n",
    "print('step 15.3: ', 'loss:', loss, 'reg:', reg)\n",
    "loss_total = loss + reg\n",
    "print('step 15.4: ', 'total loss:', loss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No difference. The model+loss combination is not impacted by batchsize. While the methods produce slightly different losses, they are all within the same range too, so no large red flags as of yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step: Recreate Dataloader\n",
    "Recreate DGL Dataloader exactly but DGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import MultipleTensors\n",
    "from utils import UnitTransformer\n",
    "\n",
    "class Cavity_2D_dataset_handling_v3():\n",
    "    def __init__(self, data_path, L=1.0, name=' ', sub_x = 1, train=True, normalize_y=False, y_normalizer=None, normalize_x = False, x_normalizer = None, up_normalizer =None, \n",
    "                 data_split = 0.7, seed = 42, vertex = False, boundaries = False):\n",
    "\n",
    "        # Normalizer settings:\n",
    "        self.normalize_y = normalize_y\n",
    "        self.normalize_x = normalize_x\n",
    "        self.y_normalizer = y_normalizer\n",
    "        self.x_normalizer = x_normalizer\n",
    "        self.up_normalizer = up_normalizer\n",
    "        self.sub_x = sub_x # <- not implemented yet\n",
    "        self.seed = seed\n",
    "        self.data_split = data_split\n",
    "        self.vertex = vertex\n",
    "        self.boundaries = boundaries\n",
    "\n",
    "        # Load in Dataset and retrieve shape\n",
    "        self.data_out   = np.load(data_path)\n",
    "        if self.sub_x > 1: self.subsampler()\n",
    "        if self.vertex: self.cell_to_vertex_converter()\n",
    "        if self.boundaries: self.add_boundaries()\n",
    "\n",
    "        print(f'Dataset Shape: {self.data_out.shape}, subsampled by {self.sub_x}')\n",
    "        # NOTE this can also be in the form of reynolds number \n",
    "        self.data_lid_v = np.round(np.arange(0.5,100.5,0.5),1) # * 0.1/0.01 <- apply for Reynolds Number\n",
    "        self.n_batches  = self.data_out.shape[0]\n",
    "        self.nx         = int(self.data_out.shape[1])\n",
    "        self.num_nodes  = self.nx**2\n",
    "\n",
    "        self.L = L\n",
    "        self.train = train\n",
    "\n",
    "        #super(Cavity_2D_dataset_handling_v3, self)#.__init__(name)   # invoke super method after read data\n",
    "        self.process()\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        # SECTION 0: Split into train or test (Same as for FNO training)\n",
    "        train_size = int(self.data_split * self.n_batches)\n",
    "        test_size = self.n_batches - train_size\n",
    "\n",
    "        seed_generator = torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        train_dataset,  test_dataset    = torch.utils.data.random_split(torch.from_numpy(self.data_out),    [train_size, test_size], generator=seed_generator)\n",
    "        train_lid_v,    test_lid_v      = torch.utils.data.random_split(torch.from_numpy(self.data_lid_v),  [train_size, test_size], generator=seed_generator)\n",
    "        \n",
    "        print(f'''Dataset Split up using torch generator seed: {seed_generator.initial_seed()}\n",
    "              This can be replicated e.g.\n",
    "                generator_object = torch.Generator().manual_seed({seed_generator.initial_seed()})\\n ''')\n",
    "        \n",
    "        # The torch.utils.data.random_split() only gives objects with the whole datset or a integers, so we need to override these variables with the indexed datset split\n",
    "        train_dataset,  test_dataset    = train_dataset.dataset[train_dataset.indices,...], test_dataset.dataset[test_dataset.indices,...]\n",
    "        train_lid_v,    test_lid_v      = train_lid_v.dataset[train_lid_v.indices], test_lid_v.dataset[test_lid_v.indices]\n",
    "\n",
    "        if self.train:\n",
    "            self.data_out   = train_dataset\n",
    "            self.data_lid_v = train_lid_v\n",
    "            self.n_batches  = train_size\n",
    "        else:\n",
    "            self.data_out   = test_dataset\n",
    "            self.data_lid_v = test_lid_v\n",
    "            self.n_batches  = test_size\n",
    "        \n",
    "        # SECTION 1: Transformer Queries\n",
    "        # Assume Isotropic Grid adjusting coordinates for cell centered or vertex points accordingly.\n",
    "        # Also includes boundaries if stated (note boundaries + cell-centered will cause boundary coordinates to be 0-dx, 1+dx overflow)\n",
    "        # this is to maintain isotropic property\n",
    "        divisor = self.nx - 2*int(self.boundaries) + 1*int(self.vertex)\n",
    "        dx = self.L/divisor\n",
    "        offset = dx/2 - dx*int(self.boundaries) + dx/2*int(self.vertex)\n",
    "        x = torch.arange(self.nx)/divisor + offset\n",
    "        y = x\n",
    "\n",
    "        # take note of the indexing. Best for this to match the output\n",
    "        [X, Y] = torch.meshgrid(x, y, indexing = 'ij')\n",
    "        print(X.shape)\n",
    "        X = X.reshape(self.num_nodes,1)\n",
    "        Y = Y.reshape(self.num_nodes,1)\n",
    "\n",
    "        # we need to linearize these matrices.\n",
    "        self.X_for_queries = torch.concat([Y,X],dim=-1)\n",
    "        print('Queries', self.X_for_queries.shape, 'Coordinates', X.shape)\n",
    "        \n",
    "        # Final Data:\n",
    "        self.queries = self.X_for_queries\n",
    "        self.theta = torch.zeros([self.n_batches])\n",
    "        self.input_f = self.data_lid_v\n",
    "        self.output_truth = self.data_out.reshape(self.n_batches, self.num_nodes,3)\n",
    "\n",
    "        \n",
    "        print('Queries:',self.queries.shape)\n",
    "        print('Theta: (Zeros)',self.theta.shape)\n",
    "        print('Input Queries:',self.input_f.shape)\n",
    "        print('Ground Truth:',self.output_truth.shape)\n",
    "\n",
    "        # SECTION 3: Transform to be MIOdataset Loader Compatible\n",
    "        self.dgl_free_processeor()\n",
    "        self.__update_dataset_config()\n",
    "\n",
    "    def subsampler(self):\n",
    "        self.data_out = torch.nn.functional.avg_pool2d(torch.tensor(self.data_out).permute(0,3,1,2), self.sub_x).permute(0,2,3,1).numpy()\n",
    "\n",
    "    def cell_to_vertex_converter(self):\n",
    "        self.data_out = torch.nn.functional.avg_pool2d(torch.tensor(self.data_out).permute(0,3,1,2),2,stride=1).permute(0,2,3,1).numpy()\n",
    "    \n",
    "    def add_boundaries(self):\n",
    "        self.data_out = torch.nn.functional.pad(torch.tensor(self.data_out).permute(0,3,1,2),(1, 1, 1, 1)).permute(0,2,3,1).numpy()\n",
    "\n",
    "        # Lid Velocity\n",
    "        self.data_out[:,-1 ,:,0] = 1\n",
    "\n",
    "        # Pressure\n",
    "        self.data_out[:,  0 ,1:-1, 2] = self.data_out[:,  1 ,1:-1, 2]  # Bottom Wall\n",
    "        self.data_out[:, -1 ,1:-1, 2] = self.data_out[:, -2 ,1:-1, 2]  # Lid (y-vel)\n",
    "        self.data_out[:,1:-1,  0 , 2] = self.data_out[:,1:-1,  1 , 2]  # Left Wall\n",
    "        self.data_out[:,1:-1, -1 , 2] = self.data_out[:,1:-1, -2 , 2]  # Right Wall\n",
    "\n",
    "    # Now we get to our own processor\n",
    "    def dgl_free_processeor(self):\n",
    "\n",
    "        if self.normalize_y:\n",
    "            print('normalizing y (output truth)')\n",
    "            self.__normalize_y()\n",
    "        if self.normalize_x:\n",
    "            print('normalizing x (input coordinates)')\n",
    "            self.__normalize_x()\n",
    "\n",
    "        self.__update_dataset_config()\n",
    "\n",
    "    # This appears to normalize the entire dataset vertically stacked\n",
    "    def __normalize_y(self):\n",
    "        if self.y_normalizer is None:\n",
    "            #y_feats_all = torch.cat([g.ndata['y'] for g in self.graphs],dim=0)\n",
    "            y_feats_all = self.output_truth.reshape(self.n_batches * self.num_nodes,3)\n",
    "            if self.normalize_y == 'unit':\n",
    "                self.y_normalizer = UnitTransformer(y_feats_all)\n",
    "                print('Target features are normalized using unit transformer')\n",
    "                print(self.y_normalizer.mean, self.y_normalizer.std)\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "        \n",
    "        self.output_truth = self.y_normalizer.transform(self.output_truth, inverse=False)\n",
    "        #for g in self.graphs:\n",
    "        #    g.ndata['y'] = self.y_normalizer.transform(g.ndata[\"y\"], inverse=False)  # a torch quantile transformer\n",
    "        print('Target features are normalized using unit transformer')\n",
    "\n",
    "    def __normalize_x(self):\n",
    "        if self.x_normalizer is None:\n",
    "            # X features are the same for all cases (same grid coords)\n",
    "            #x_feats_all = torch.cat([g.ndata[\"x\"] for g in self.graphs],dim=0)\n",
    "            x_feats_all = self.queries\n",
    "            if self.normalize_x == 'unit':\n",
    "                self.x_normalizer = UnitTransformer(x_feats_all)\n",
    "                self.up_normalizer = UnitTransformer(self.theta)\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "\n",
    "        #for g in self.graphs:\n",
    "        #    g.ndata['x'] = self.x_normalizer.transform(g.ndata['x'], inverse=False)\n",
    "        self.queries = self.x_normalizer.transform(self.queries, inverse=False)\n",
    "        self.u_p_list = self.up_normalizer.transform(self.theta, inverse=False)\n",
    "        print('Input features are normalized using unit transformer')\n",
    "\n",
    "    def __update_dataset_config(self):\n",
    "        self.config = {\n",
    "            'input_dim': self.queries.shape[-1],\n",
    "            'theta_dim': 1,\n",
    "            'output_dim': self.output_truth.shape[-1],\n",
    "            'branch_sizes': [1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (200, 65, 65, 3), subsampled by 4\n",
      "Dataset Split up using torch generator seed: 42\n",
      "              This can be replicated e.g.\n",
      "                generator_object = torch.Generator().manual_seed(42)\n",
      " \n",
      "torch.Size([65, 65])\n",
      "Queries torch.Size([4225, 2]) Coordinates torch.Size([4225, 1])\n",
      "Queries: torch.Size([4225, 2])\n",
      "Theta: (Zeros) torch.Size([140])\n",
      "Input Queries: torch.Size([140])\n",
      "Ground Truth: torch.Size([140, 4225, 3])\n",
      "normalizing y (output truth)\n",
      "Target features are normalized using unit transformer\n",
      "tensor([[ 0.0093,  0.0004, -0.0328]], dtype=torch.float64) tensor([[0.2326, 0.1816, 0.1040]], dtype=torch.float64)\n",
      "Target features are normalized using unit transformer\n",
      "normalizing x (input coordinates)\n",
      "Input features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "dgl_free_dataset = Cavity_2D_dataset_handling_v3(dataset_args['file'], name='cavity', train=True, sub_x = dataset_args['subsampler'],\n",
    "                                    normalize_y=dataset_args['use-normalizer'], normalize_x = dataset_args['normalize_x'],\n",
    "                                    data_split = dataset_args['percent split (decimal)'], seed = dataset_args['randomizer seed'],\n",
    "                                    vertex = dataset_args['cell to pointwise'], boundaries = dataset_args['add boundaries']\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare unit normalizer statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl free (x): tensor([[0.5000, 0.5000]]) tensor([[0.2932, 0.2932]])\n",
      "dgl      (x): tensor([[0.5000, 0.5000]]) tensor([[0.2932, 0.2932]])\n"
     ]
    }
   ],
   "source": [
    "print('dgl free (x):',dgl_free_dataset.x_normalizer.mean, dgl_free_dataset.x_normalizer.std)\n",
    "print('dgl      (x):',dgl_dataset.x_normalizer.mean, dgl_dataset.x_normalizer.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl free (y): tensor([[ 0.0093,  0.0004, -0.0328]], dtype=torch.float64) tensor([[0.2326, 0.1816, 0.1040]], dtype=torch.float64)\n",
      "dgl      (y): tensor([[ 0.0093,  0.0004, -0.0328]]) tensor([[0.2326, 0.1816, 0.1040]])\n"
     ]
    }
   ],
   "source": [
    "print('dgl free (y):',dgl_free_dataset.y_normalizer.mean, dgl_free_dataset.y_normalizer.std)\n",
    "print('dgl      (y):',dgl_dataset.y_normalizer.mean, dgl_dataset.y_normalizer.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl free (u_p): tensor([0.]) tensor([1.0000e-08])\n",
      "dgl      (u_p): tensor([[0.]]) tensor([[1.0000e-08]])\n"
     ]
    }
   ],
   "source": [
    "print('dgl free (u_p):',dgl_free_dataset.up_normalizer.mean, dgl_free_dataset.up_normalizer.std)\n",
    "print('dgl      (u_p):',dgl_dataset.up_normalizer.mean, dgl_dataset.up_normalizer.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all match! all though to be fair, in previous iterations I may not have transformed them correctly. So this may be worth looking at. Lets check the normalizer statistics from our Artemis file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (200, 65, 65, 3), subsampled by 4\n",
      "Dataset Split up using torch generator seed: 42\n",
      "              This can be replicated e.g.\n",
      "                generator_object = torch.Generator().manual_seed(42)\n",
      " \n",
      "torch.Size([65, 65])\n",
      "Target features are normalized using unit transformer\n",
      "tensor([[ 0.0093,  0.0004, -0.0328]], dtype=torch.float64) tensor([[0.2326, 0.1816, 0.1040]], dtype=torch.float64)\n",
      "Target features are normalized using unit transformer\n",
      "Input features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "from train_cavity_artemis import Cavity_2D_dataset_for_GNOT\n",
    "dataset_args = dict()\n",
    "\n",
    "# 1. Prepare Data\n",
    "dataset_args['file']                    = r'C:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\data\\steady_cavity_case_b200_maxU100ms_simple_normalized.npy'\n",
    "dataset_args['percent split (decimal)'] = 0.7\n",
    "dataset_args['randomizer seed']         = 42\n",
    "dataset_args['Interpolate (instead of Extrapolate)'] = True\n",
    "dataset_args['use-normalizer']          = 'unit'\n",
    "dataset_args['normalize_x']             = 'unit'\n",
    "dataset_args['cell to pointwise']       = True\n",
    "dataset_args['add boundaries']          = True\n",
    "dataset_args['sub_x']                   = 4\n",
    "\n",
    "dataset = Cavity_2D_dataset_for_GNOT(data_path=dataset_args['file'], \n",
    "                                    L=1.0, \n",
    "                                    sub_x = dataset_args['sub_x'], \n",
    "                                    train=True, \n",
    "                                    normalize_y=dataset_args['use-normalizer'], \n",
    "                                    y_normalizer=None, \n",
    "                                    normalize_x = dataset_args['normalize_x'], \n",
    "                                    x_normalizer = None, \n",
    "                                    up_normalizer =None, \n",
    "                                    vertex = dataset_args['cell to pointwise'], \n",
    "                                    boundaries = dataset_args['add boundaries'])\n",
    "# Process dataset\n",
    "dataset.assign_data_split_type(inference=dataset_args['Interpolate (instead of Extrapolate)'], \n",
    "                                train_ratio=dataset_args['percent split (decimal)'], \n",
    "                                seed=dataset_args['randomizer seed'])\n",
    "dataset.process(theta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl free (x): tensor([[0.5000, 0.5000]]) tensor([[0.2932, 0.2932]])\n",
      "dgl      (x): tensor([[0.5000, 0.5000]]) tensor([[0.2932, 0.2932]])\n",
      "artemis  (x): tensor([[0.5000, 0.5000]]) tensor([[0.2932, 0.2932]])\n"
     ]
    }
   ],
   "source": [
    "print('dgl free (x):',dgl_free_dataset.x_normalizer.mean, dgl_free_dataset.x_normalizer.std)\n",
    "print('dgl      (x):',dgl_dataset.x_normalizer.mean, dgl_dataset.x_normalizer.std)\n",
    "print('artemis  (x):',dataset.x_normalizer.mean, dataset.x_normalizer.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl free (y): tensor([[ 0.0093,  0.0004, -0.0328]], dtype=torch.float64) tensor([[0.2326, 0.1816, 0.1040]], dtype=torch.float64)\n",
      "dgl      (y): tensor([[ 0.0093,  0.0004, -0.0328]]) tensor([[0.2326, 0.1816, 0.1040]])\n",
      "artemis  (y): tensor([[ 0.0093,  0.0004, -0.0328]], dtype=torch.float64) tensor([[0.2326, 0.1816, 0.1040]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print('dgl free (y):',dgl_free_dataset.y_normalizer.mean, dgl_free_dataset.y_normalizer.std)\n",
    "print('dgl      (y):',dgl_dataset.y_normalizer.mean, dgl_dataset.y_normalizer.std)\n",
    "print('artemis  (y):',dataset.y_normalizer.mean, dataset.y_normalizer.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you got. There is an error the ground truth dataset is not normalized properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl dataset normalized: tensor([ 0.1599, -1.6523])\n",
      "artemis dataset \"normalized\": tensor([ 0.1599, -1.6521])\n"
     ]
    }
   ],
   "source": [
    "g1_b = dgl.unbatch(g1)\n",
    "g1_b[0].ndata['x'].shape\n",
    "\n",
    "# query random point:\n",
    "print('dgl dataset normalized:', g1_b[0].ndata['x'][100,:])\n",
    "\n",
    "print('artemis dataset \"normalized\":',dataset.queries[100,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, it looks like the code itself did not actually transform the dataset! Fixing this might cause the model to train again properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets go in and fix this in the artemis code and retry this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it looks to match! Lets run the artemis training file locally again and hope for the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train_cavity_artemis.py\", line 404, in <module>\n",
      "    dataset = CavityDataset(dataset=dataset)\n",
      "  File \"train_cavity_artemis.py\", line 234, in __init__\n",
      "    self.in_queries = self.queries\n",
      "AttributeError: 'CavityDataset' object has no attribute 'queries'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (200, 65, 65, 3), subsampled by 4\n",
      "Dataset Split up using torch generator seed: 42\n",
      "              This can be replicated e.g.\n",
      "                generator_object = torch.Generator().manual_seed(42)\n",
      " \n",
      "torch.Size([65, 65])\n",
      "Target features are normalized using unit transformer\n",
      "tensor([[ 0.0093,  0.0004, -0.0328]], dtype=torch.float64) tensor([[0.2326, 0.1816, 0.1040]], dtype=torch.float64)\n",
      "Target features are normalized using unit transformer\n",
      "Input features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "!python train_cavity_artemis.py --theta False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cavity_2D_dataset_for_GNOT():\n",
    "    def __init__(self, data_path, L=1.0, sub_x = 1, train=True, normalize_y=False, y_normalizer=None, normalize_x = False, x_normalizer = None, up_normalizer =None, vertex = False, boundaries = False):\n",
    "\n",
    "        '''\n",
    "        This class takes in a dataset structure used by FNO and PINO models. \n",
    "        The dataset is typically stored as [Batch, Timesteps, X-coords, Y-coords]\n",
    "        This function creates a Mesh Grid of coordinates (which is the same for all batches)\n",
    "        and reshapes it into a linear with shape [Batch, number of nodes, dims(coordinates)].\n",
    "        NOTE This array will then be the Transformer Queries Array (X or g).\n",
    "\n",
    "        Similarly, the ground truth voriticity profiles are taken and split by ic_t_steps into\n",
    "        both the output shape/ training validation loss dataset (Y) and the initial conditions \n",
    "        which will be the first ic_t_steps time steps in the dataset.\n",
    "        NOTE This ic array will be nested in a tuple and called as an input function\n",
    "        i.e. The Transformer Keys and Values Array (f or g_u).\n",
    "\n",
    "        Finally, Theta is set a as a small zero array as the current model handling does not take\n",
    "        a null input. This should not impact the model training (although it does make the datasets\n",
    "        slighly larger which is not ideal for memory)\n",
    "        '''\n",
    "        # Normalizer settings:\n",
    "        self.normalize_y = normalize_y\n",
    "        self.normalize_x = normalize_x\n",
    "        self.y_normalizer = y_normalizer\n",
    "        self.x_normalizer = x_normalizer\n",
    "        self.up_normalizer = up_normalizer\n",
    "        self.sub_x = sub_x\n",
    "        self.vertex = vertex\n",
    "        self.boundaries = boundaries\n",
    "\n",
    "        # Load in Dataset and retrieve shape\n",
    "        self.data_out   = np.load(data_path)\n",
    "        if self.sub_x > 1: self.subsampler()\n",
    "        if self.vertex: self.cell_to_vertex_converter()\n",
    "        if self.boundaries: self.add_boundaries()\n",
    "\n",
    "        print(f'Dataset Shape: {self.data_out.shape}, subsampled by {self.sub_x}')\n",
    "        # NOTE this can also be in the form of reynolds number \n",
    "        self.data_lid_v = np.round(np.arange(0.5,100.5,0.5),1) * 0.1/0.01 #<- apply for Reynolds Number\n",
    "        self.n_batches  = self.data_out.shape[0]\n",
    "        self.nx         = int(self.data_out.shape[1])\n",
    "        self.num_nodes  = self.nx**2\n",
    "\n",
    "        self.L = L\n",
    "        self.train = train\n",
    "\n",
    "    def assign_data_split_type(self, inference=True, train_ratio=0.7, seed=42):\n",
    "        self.seed = seed\n",
    "        self.data_split = train_ratio\n",
    "        self.inference = inference\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        # SECTION 0: Split into train or test (Same as for FNO training)\n",
    "        train_size = int(self.data_split * self.n_batches)\n",
    "        test_size = self.n_batches - train_size\n",
    "\n",
    "        seed_generator = torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "        # Perform Inference or Extrapolation (Inference is randomly sampled)\n",
    "        if self.inference:\n",
    "            train_dataset,  test_dataset    = torch.utils.data.random_split(torch.from_numpy(self.data_out),    [train_size, test_size], generator=seed_generator)\n",
    "            train_lid_v,    test_lid_v      = torch.utils.data.random_split(torch.from_numpy(self.data_lid_v),  [train_size, test_size], generator=seed_generator)\n",
    "            \n",
    "            # The torch.utils.data.random_split() only gives objects with the whole datset or a integers, so we need to override these variables with the indexed datset split\n",
    "            train_dataset,  test_dataset    = train_dataset.dataset[train_dataset.indices,...], test_dataset.dataset[test_dataset.indices,...]\n",
    "            train_lid_v,    test_lid_v      = train_lid_v.dataset[train_lid_v.indices], test_lid_v.dataset[test_lid_v.indices]\n",
    "            print(f'''Dataset Split up using torch generator seed: {seed_generator.initial_seed()}\n",
    "              This can be replicated e.g.\n",
    "                generator_object = torch.Generator().manual_seed({seed_generator.initial_seed()})\\n ''')\n",
    "        else:\n",
    "            train_dataset,  test_dataset    = torch.from_numpy(self.data_out[:train_size,...]), torch.from_numpy(self.data_out[train_size:,...])\n",
    "            train_lid_v,    test_lid_v      = torch.from_numpy(self.data_out[:test_size,...]), torch.from_numpy(self.data_out[test_size:,...])\n",
    "\n",
    "        \n",
    "\n",
    "        if self.train:\n",
    "            self.data_out   = train_dataset\n",
    "            self.data_lid_v = train_lid_v\n",
    "            self.n_batches  = train_size\n",
    "        else:\n",
    "            self.data_out   = test_dataset\n",
    "            self.data_lid_v = test_lid_v\n",
    "            self.n_batches  = test_size\n",
    "\n",
    "        # SECTION 1: Transformer Queries\n",
    "        # Assume Isotropic Grid adjusting coordinates for cell centered or vertex points accordingly.\n",
    "        # Also includes boundaries if stated (note boundaries + cell-centered will cause boundary coordinates to be 0-dx, 1+dx overflow)\n",
    "        # this is to maintain isotropic property\n",
    "        divisor = self.nx - 2*int(self.boundaries) + 1*int(self.vertex)\n",
    "        dx = self.L/divisor\n",
    "        offset = dx/2 - dx*int(self.boundaries) + dx/2*int(self.vertex)\n",
    "        x = torch.arange(self.nx)/divisor + offset\n",
    "        y = x\n",
    "\n",
    "        # take note of the indexing. Best for this to match the output\n",
    "        [X, Y] = torch.meshgrid(x, y, indexing = 'ij')\n",
    "        print(X.shape)\n",
    "        X = X.reshape(self.num_nodes,1)\n",
    "        Y = Y.reshape(self.num_nodes,1)\n",
    "\n",
    "        # we need to linearize these matrices.\n",
    "        self.X_for_queries = torch.concat([Y,X],dim=-1)\n",
    "        print('Queries', self.X_for_queries.shape, 'Coordinates', X.shape)\n",
    "        \n",
    "        # SECTION 3: Transform to be MIOdataset Loader Compatible\n",
    "        self.normalizer()\n",
    "        self.__update_dataset_config()\n",
    "\n",
    "    def subsampler(self):\n",
    "        self.data_out = torch.nn.functional.avg_pool2d(torch.tensor(self.data_out).permute(0,3,1,2), self.sub_x).permute(0,2,3,1).numpy()\n",
    "\n",
    "    def cell_to_vertex_converter(self):\n",
    "        self.data_out = torch.nn.functional.avg_pool2d(torch.tensor(self.data_out).permute(0,3,1,2),2,stride=1).permute(0,2,3,1).numpy()\n",
    "    \n",
    "    def add_boundaries(self):\n",
    "        self.data_out = torch.nn.functional.pad(torch.tensor(self.data_out).permute(0,3,1,2),(1, 1, 1, 1)).permute(0,2,3,1).numpy()\n",
    "\n",
    "        # Lid Velocity\n",
    "        self.data_out[:,-1 ,:,0] = 1\n",
    "\n",
    "        # Pressure\n",
    "        self.data_out[:,  0 ,1:-1, 2] = self.data_out[:,  1 ,1:-1, 2]  # Bottom Wall\n",
    "        self.data_out[:, -1 ,1:-1, 2] = self.data_out[:, -2 ,1:-1, 2]  # Lid (y-vel)\n",
    "        self.data_out[:,1:-1,  0 , 2] = self.data_out[:,1:-1,  1 , 2]  # Left Wall\n",
    "        self.data_out[:,1:-1, -1 , 2] = self.data_out[:,1:-1, -2 , 2]  # Right Wall\n",
    "\n",
    "    def normalizer(self):\n",
    "        if self.normalize_y:\n",
    "            self.__normalize_y()\n",
    "        if self.normalize_x:\n",
    "            self.__normalize_x()\n",
    "\n",
    "        self.__update_dataset_config()\n",
    "        \n",
    "\n",
    "    def __normalize_y(self):\n",
    "        if self.y_normalizer is None:\n",
    "            if self.normalize_y == 'unit':\n",
    "                self.y_normalizer = UnitTransformer(self.data_out)\n",
    "                print('Target features are normalized using unit transformer')\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            self.data_out = self.y_normalizer.transform(self.data_out, inverse=False)  # a torch quantile transformer\n",
    "            print('Target features are normalized using unit transformer')\n",
    "\n",
    "    def __normalize_x(self):\n",
    "        if self.x_normalizer is None:\n",
    "            if self.normalize_x == 'unit':\n",
    "                self.x_normalizer = UnitTransformer(self.X_for_queries)\n",
    "                self.up_normalizer = UnitTransformer(self.data_lid_v)\n",
    "            else: \n",
    "                raise NotImplementedError\n",
    "\n",
    "    def __update_dataset_config(self):\n",
    "\n",
    "        self.config = {\n",
    "            'input_dim': self.X_for_queries.shape[-1],\n",
    "            #'theta_dim': self.data_lid_v.shape[1],\n",
    "            'output_dim': self.data_out.shape[-1]#,\n",
    "            #'branch_sizes': [x.shape[1] for x in self.inputs_f[0]] if isinstance(self.inputs_f, list) else 0\n",
    "        }\n",
    "\n",
    "class UnitTransformer():\n",
    "    def __init__(self, X):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, keepdim=True) + 1e-8\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        self.mean = self.mean.to(device)\n",
    "        self.std = self.std.to(device)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, inverse=True,component='all'):\n",
    "        if component == 'all' or 'all-reduce':\n",
    "            if inverse:\n",
    "                orig_shape = X.shape\n",
    "                return (X*(self.std - 1e-8) + self.mean).view(orig_shape)\n",
    "            else:\n",
    "                return (X-self.mean)/self.std\n",
    "        else:\n",
    "            if inverse:\n",
    "                orig_shape = X.shape\n",
    "                return (X*(self.std[:,component] - 1e-8)+ self.mean[:,component]).view(orig_shape)\n",
    "            else:\n",
    "                return (X - self.mean[:,component])/self.std[:,component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (200, 33, 33, 3), subsampled by 8\n"
     ]
    }
   ],
   "source": [
    "data = Cavity_2D_dataset_for_GNOT(data_path= r'C:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\data\\steady_cavity_case_b200_maxU100ms_simple_normalized.npy', \n",
    "                                    L=1.0, \n",
    "                                    sub_x = 8, \n",
    "                                    train=True, \n",
    "                                    normalize_y='unit', \n",
    "                                    y_normalizer=None, \n",
    "                                    normalize_x = 'unit', \n",
    "                                    x_normalizer = None, \n",
    "                                    up_normalizer =None, \n",
    "                                    vertex = True, \n",
    "                                    boundaries = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split up using torch generator seed: 42\n",
      "              This can be replicated e.g.\n",
      "                generator_object = torch.Generator().manual_seed(42)\n",
      " \n",
      "torch.Size([33, 33])\n",
      "Queries torch.Size([1089, 2]) Coordinates torch.Size([1089, 1])\n",
      "Target features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "data.assign_data_split_type(inference=True, train_ratio=0.7, seed=42)\n",
    "data.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 33, 33, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate input/output with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.noahs_model import CGPTNO\n",
    "\n",
    "# 2. Construct Model\n",
    "model_args = dict()\n",
    "model_args['trunk_size']        = data.config['input_dim']\n",
    "#model_args['theta_size']        = data.config['theta_dim']\n",
    "model_args['branch_sizes']      = [1]\n",
    "\n",
    "model_args['output_size']         = 3\n",
    "model_args['n_layers']            = 3\n",
    "model_args['n_hidden']            = 128  \n",
    "model_args['n_head']              = 1\n",
    "model_args['attn_type']           = 'linear'\n",
    "model_args['ffn_dropout']         = 0.0\n",
    "model_args['attn_dropout']        = 0.0\n",
    "model_args['mlp_layers']          = 2\n",
    "model_args['act']                 = 'gelu'\n",
    "model_args['hfourier_dim']        = 0\n",
    "\n",
    "model = None\n",
    "model = CGPTNO(\n",
    "            trunk_size          = model_args['trunk_size'],# + model_args['theta_size'],\n",
    "            branch_sizes        = model_args['branch_sizes'],     # No input function means no branches\n",
    "            output_size         = model_args['output_size'],\n",
    "            n_layers            = model_args['n_layers'],\n",
    "            n_hidden            = model_args['n_hidden'],\n",
    "            n_head              = model_args['n_head'],\n",
    "            attn_type           = model_args['attn_type'],\n",
    "            ffn_dropout         = model_args['ffn_dropout'],\n",
    "            attn_dropout        = model_args['attn_dropout'],\n",
    "            mlp_layers          = model_args['mlp_layers'],\n",
    "            act                 = model_args['act'],\n",
    "            horiz_fourier_dim   = model_args['hfourier_dim']\n",
    "            )#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data_lid_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data_lid_v.unsqueeze(-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 1089, 128]), z.shape: torch.Size([1, 1, 128])\n",
      "x.shape: torch.Size([1, 1089, 128]), z.shape: torch.Size([1, 1, 128])\n",
      "x.shape: torch.Size([1, 1089, 128]), z.shape: torch.Size([1, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "out = model(x=data.X_for_queries.unsqueeze(0),inputs = data.data_lid_v[0].unsqueeze(0).unsqueeze(0).unsqueeze(0).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(data.data_lid_v[0].unsqueeze(0).unsqueeze(0).unsqueeze(0).float().shape)\n",
    "print(data.data_lid_v[0].reshape([1,1,1]).float().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (200, 65, 65, 3), subsampled by 4\n",
      "Dataset Split up using torch generator seed: 42\n",
      "              This can be replicated e.g.\n",
      "                generator_object = torch.Generator().manual_seed(42)\n",
      " \n",
      "torch.Size([65, 65])\n",
      "Queries torch.Size([4225, 2]) Coordinates torch.Size([4225, 1])\n",
      "Target features are normalized using unit transformer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noahc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\dgl\\heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[ 0.0093,  0.0004, -0.0328]]) tensor([[0.2326, 0.1816, 0.1040]])\n",
      "Target features are normalized using unit transformer\n",
      "Input features are normalized using unit transformer\n"
     ]
    }
   ],
   "source": [
    "from data_storage.cavity_2d_data_handling import Cavity_2D_dataset_handling_v2\n",
    "\n",
    "dataset_args = dict()\n",
    "training_args = dict()\n",
    "\n",
    "# 1. Prepare Data\n",
    "dataset_args['file']                    = r'C:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\data\\steady_cavity_case_b200_maxU100ms_simple_normalized.npy'\n",
    "dataset_args['percent split (decimal)'] = 0.7\n",
    "dataset_args['randomizer seed']         = 42\n",
    "dataset_args['use-normalizer']          = 'unit'\n",
    "dataset_args['normalize_x']             = 'unit'\n",
    "#dataset_args['subsampler']              = 4\n",
    "dataset_args['cell to pointwise']       = True\n",
    "dataset_args['add boundaries']          = True\n",
    "\n",
    "dataset = Cavity_2D_dataset_handling_v2(dataset_args['file'], name='cavity', train=True, sub_x = 4,\n",
    "                                    normalize_y=dataset_args['use-normalizer'], normalize_x = dataset_args['normalize_x'],\n",
    "                                    data_split = dataset_args['percent split (decimal)'], seed = dataset_args['randomizer seed'],\n",
    "                                    vertex = dataset_args['cell to pointwise'], boundaries = dataset_args['add boundaries']\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import MultipleTensors, MIODataLoader\n",
    "training_dataloader = MIODataLoader(dataset, batch_size=1, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_n, batch in enumerate(training_dataloader):\n",
    "    a,b,c = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4225, 2])\n",
      "torch.Size([1, 1, 1])\n",
      "compared to\n",
      "torch.Size([1089, 2])\n",
      "torch.Size([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(a.ndata['x'].shape)\n",
    "print(c[0].shape)\n",
    "\n",
    "print('compared to')\n",
    "\n",
    "print(data.X_for_queries.shape)\n",
    "print(data.data_lid_v[0].unsqueeze(0).unsqueeze(0).unsqueeze(0).float().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a match. The model works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok nice. Lets write some functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_model_args():\n",
    "\n",
    "    model_args = dict()\n",
    "    model_args['trunk_size']        = data.config['input_dim']\n",
    "    #model_args['theta_size']        = data.config['theta_dim']\n",
    "    model_args['branch_sizes']      = [1]\n",
    "\n",
    "    model_args['output_size']         = 3\n",
    "    model_args['n_layers']            = 3\n",
    "    model_args['n_hidden']            = 128  \n",
    "    model_args['n_head']              = 1\n",
    "    model_args['attn_type']           = 'linear'\n",
    "    model_args['ffn_dropout']         = 0.0\n",
    "    model_args['attn_dropout']        = 0.0\n",
    "    model_args['mlp_layers']          = 2\n",
    "    model_args['act']                 = 'gelu'\n",
    "    model_args['hfourier_dim']        = 0\n",
    "\n",
    "    return model_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 140)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(dataset.data_out.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data_lid_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a trial dictionary output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.load(r'C:\\Users\\Noahc\\Documents\\USYD\\PHD\\8 - Github\\GNOT\\checkpoints\\gnot_artemis\\test_results.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model Configuration': {'trunk_size': 2,\n",
       "  'branch_sizes': [1],\n",
       "  'output_size': 3,\n",
       "  'n_layers': 3,\n",
       "  'n_hidden': 128,\n",
       "  'n_head': 1,\n",
       "  'attn_type': 'linear',\n",
       "  'ffn_dropout': 0.0,\n",
       "  'attn_dropout': 0.0,\n",
       "  'mlp_layers': 2,\n",
       "  'act': 'gelu',\n",
       "  'hfourier_dim': 0},\n",
       " 'Training Configuration': {'epochs': 1,\n",
       "  'save_dir': 'gnot_artemis',\n",
       "  'base_lr': 0.001,\n",
       "  'weight-decay': 5e-05,\n",
       "  'grad-clip': 1000.0,\n",
       "  'save_name': 'test'},\n",
       " 'Data Configuration': {'file': 'C:\\\\Users\\\\Noahc\\\\Documents\\\\USYD\\\\PHD\\\\8 - Github\\\\GNOT\\\\data\\\\steady_cavity_case_b200_maxU100ms_simple_normalized.npy',\n",
       "  'percent split (decimal)': 0.7,\n",
       "  'randomizer seed': 42,\n",
       "  'Interpolate (instead of Extrapolate)': True,\n",
       "  'use-normalizer': 'unit',\n",
       "  'normalize_x': 'unit',\n",
       "  'cell to pointwise': True,\n",
       "  'add boundaries': True,\n",
       "  'sub_x': 4},\n",
       " 'Epoch Time': [0.5232278999999997],\n",
       " 'Training L2 Loss': [0.0700981542468071]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
